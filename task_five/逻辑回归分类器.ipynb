{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row,functions\n",
    "from pyspark.ml.linalg import Vector,Vectors\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer,HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression,LogisticRegressionModel,BinaryLogisticRegressionSummary, LogisticRegression\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"logistics regression\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "# 用于将数据集特征和label切分开\n",
    "def f(x):\n",
    "    rel = {}\n",
    "    rel['features'] = Vectors.dense(float(x[0]),float(x[1]),float(x[2]),float(x[3]))\n",
    "    rel['label'] = str(x[4])\n",
    "    return rel\n",
    " \n",
    "data = spark.sparkContext.textFile(\"file:///home/mayfly/workstation/mycode/spark_study/iris.txt\").map(lambda line: line.split(',')).map(lambda p: Row(**f(p)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(features=DenseVector([5.1, 3.5, 1.4, 0.2]), label='Iris-setosa')\n",
      "Row(features=DenseVector([4.9, 3.0, 1.4, 0.2]), label='Iris-setosa')\n",
      "Row(features=DenseVector([4.7, 3.2, 1.3, 0.2]), label='Iris-setosa')\n",
      "Row(features=DenseVector([4.6, 3.1, 1.5, 0.2]), label='Iris-setosa')\n",
      "Row(features=DenseVector([5.0, 3.6, 1.4, 0.2]), label='Iris-setosa')\n",
      "Row(features=DenseVector([5.4, 3.9, 1.7, 0.4]), label='Iris-setosa')\n"
     ]
    }
   ],
   "source": [
    "for index, t in enumerate(data.collect()):\n",
    "    if index>5:\n",
    "        break\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将RDD转换为dataframe\n",
    "data = data.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-versicolor:[7.0,3.2,4.7,1.4]\n",
      "Iris-versicolor:[6.4,3.2,4.5,1.5]\n",
      "Iris-versicolor:[6.9,3.1,4.9,1.5]\n",
      "Iris-versicolor:[5.5,2.3,4.0,1.3]\n",
      "Iris-versicolor:[6.5,2.8,4.6,1.5]\n",
      "Iris-versicolor:[5.7,2.8,4.5,1.3]\n",
      "Iris-versicolor:[6.3,3.3,4.7,1.6]\n",
      "Iris-versicolor:[4.9,2.4,3.3,1.0]\n",
      "Iris-versicolor:[6.6,2.9,4.6,1.3]\n",
      "Iris-versicolor:[5.2,2.7,3.9,1.4]\n",
      "Iris-versicolor:[5.0,2.0,3.5,1.0]\n",
      "Iris-versicolor:[5.9,3.0,4.2,1.5]\n",
      "Iris-versicolor:[6.0,2.2,4.0,1.0]\n",
      "Iris-versicolor:[6.1,2.9,4.7,1.4]\n",
      "Iris-versicolor:[5.6,2.9,3.6,1.3]\n",
      "Iris-versicolor:[6.7,3.1,4.4,1.4]\n",
      "Iris-versicolor:[5.6,3.0,4.5,1.5]\n",
      "Iris-versicolor:[5.8,2.7,4.1,1.0]\n",
      "Iris-versicolor:[6.2,2.2,4.5,1.5]\n",
      "Iris-versicolor:[5.6,2.5,3.9,1.1]\n",
      "Iris-versicolor:[5.9,3.2,4.8,1.8]\n",
      "Iris-versicolor:[6.1,2.8,4.0,1.3]\n",
      "Iris-versicolor:[6.3,2.5,4.9,1.5]\n",
      "Iris-versicolor:[6.1,2.8,4.7,1.2]\n",
      "Iris-versicolor:[6.4,2.9,4.3,1.3]\n",
      "Iris-versicolor:[6.6,3.0,4.4,1.4]\n",
      "Iris-versicolor:[6.8,2.8,4.8,1.4]\n",
      "Iris-versicolor:[6.7,3.0,5.0,1.7]\n",
      "Iris-versicolor:[6.0,2.9,4.5,1.5]\n",
      "Iris-versicolor:[5.7,2.6,3.5,1.0]\n",
      "Iris-versicolor:[5.5,2.4,3.8,1.1]\n",
      "Iris-versicolor:[5.5,2.4,3.7,1.0]\n",
      "Iris-versicolor:[5.8,2.7,3.9,1.2]\n",
      "Iris-versicolor:[6.0,2.7,5.1,1.6]\n",
      "Iris-versicolor:[5.4,3.0,4.5,1.5]\n",
      "Iris-versicolor:[6.0,3.4,4.5,1.6]\n",
      "Iris-versicolor:[6.7,3.1,4.7,1.5]\n",
      "Iris-versicolor:[6.3,2.3,4.4,1.3]\n",
      "Iris-versicolor:[5.6,3.0,4.1,1.3]\n",
      "Iris-versicolor:[5.5,2.5,4.0,1.3]\n",
      "Iris-versicolor:[5.5,2.6,4.4,1.2]\n",
      "Iris-versicolor:[6.1,3.0,4.6,1.4]\n",
      "Iris-versicolor:[5.8,2.6,4.0,1.2]\n",
      "Iris-versicolor:[5.0,2.3,3.3,1.0]\n",
      "Iris-versicolor:[5.6,2.7,4.2,1.3]\n",
      "Iris-versicolor:[5.7,3.0,4.2,1.2]\n",
      "Iris-versicolor:[5.7,2.9,4.2,1.3]\n",
      "Iris-versicolor:[6.2,2.9,4.3,1.3]\n",
      "Iris-versicolor:[5.1,2.5,3.0,1.1]\n",
      "Iris-versicolor:[5.7,2.8,4.1,1.3]\n",
      "Iris-virginica:[6.3,3.3,6.0,2.5]\n",
      "Iris-virginica:[5.8,2.7,5.1,1.9]\n",
      "Iris-virginica:[7.1,3.0,5.9,2.1]\n",
      "Iris-virginica:[6.3,2.9,5.6,1.8]\n",
      "Iris-virginica:[6.5,3.0,5.8,2.2]\n",
      "Iris-virginica:[7.6,3.0,6.6,2.1]\n",
      "Iris-virginica:[4.9,2.5,4.5,1.7]\n",
      "Iris-virginica:[7.3,2.9,6.3,1.8]\n",
      "Iris-virginica:[6.7,2.5,5.8,1.8]\n",
      "Iris-virginica:[7.2,3.6,6.1,2.5]\n",
      "Iris-virginica:[6.5,3.2,5.1,2.0]\n",
      "Iris-virginica:[6.4,2.7,5.3,1.9]\n",
      "Iris-virginica:[6.8,3.0,5.5,2.1]\n",
      "Iris-virginica:[5.7,2.5,5.0,2.0]\n",
      "Iris-virginica:[5.8,2.8,5.1,2.4]\n",
      "Iris-virginica:[6.4,3.2,5.3,2.3]\n",
      "Iris-virginica:[6.5,3.0,5.5,1.8]\n",
      "Iris-virginica:[7.7,3.8,6.7,2.2]\n",
      "Iris-virginica:[7.7,2.6,6.9,2.3]\n",
      "Iris-virginica:[6.0,2.2,5.0,1.5]\n",
      "Iris-virginica:[6.9,3.2,5.7,2.3]\n",
      "Iris-virginica:[5.6,2.8,4.9,2.0]\n",
      "Iris-virginica:[7.7,2.8,6.7,2.0]\n",
      "Iris-virginica:[6.3,2.7,4.9,1.8]\n",
      "Iris-virginica:[6.7,3.3,5.7,2.1]\n",
      "Iris-virginica:[7.2,3.2,6.0,1.8]\n",
      "Iris-virginica:[6.2,2.8,4.8,1.8]\n",
      "Iris-virginica:[6.1,3.0,4.9,1.8]\n",
      "Iris-virginica:[6.4,2.8,5.6,2.1]\n",
      "Iris-virginica:[7.2,3.0,5.8,1.6]\n",
      "Iris-virginica:[7.4,2.8,6.1,1.9]\n",
      "Iris-virginica:[7.9,3.8,6.4,2.0]\n",
      "Iris-virginica:[6.4,2.8,5.6,2.2]\n",
      "Iris-virginica:[6.3,2.8,5.1,1.5]\n",
      "Iris-virginica:[6.1,2.6,5.6,1.4]\n",
      "Iris-virginica:[7.7,3.0,6.1,2.3]\n",
      "Iris-virginica:[6.3,3.4,5.6,2.4]\n",
      "Iris-virginica:[6.4,3.1,5.5,1.8]\n",
      "Iris-virginica:[6.0,3.0,4.8,1.8]\n",
      "Iris-virginica:[6.9,3.1,5.4,2.1]\n",
      "Iris-virginica:[6.7,3.1,5.6,2.4]\n",
      "Iris-virginica:[6.9,3.1,5.1,2.3]\n",
      "Iris-virginica:[5.8,2.7,5.1,1.9]\n",
      "Iris-virginica:[6.8,3.2,5.9,2.3]\n",
      "Iris-virginica:[6.7,3.3,5.7,2.5]\n",
      "Iris-virginica:[6.7,3.0,5.2,2.3]\n",
      "Iris-virginica:[6.3,2.5,5.0,1.9]\n",
      "Iris-virginica:[6.5,3.0,5.2,2.0]\n",
      "Iris-virginica:[6.2,3.4,5.4,2.3]\n",
      "Iris-virginica:[5.9,3.0,5.1,1.8]\n"
     ]
    }
   ],
   "source": [
    "# 把刚刚得到的数据注册成一个表iris\n",
    "data.createOrReplaceTempView(\"iris\")\n",
    "# 通过sql语句进行数据查询，由于不需要全部的三分类，所以去除一个分类\n",
    "df = spark.sql(\"select * from iris where label != 'Iris-setosa'\")\n",
    "# 转换成rdd输出查看\n",
    "rel = df.rdd.map(lambda t : str(t[1])+\":\"+str(t[0])).collect()\n",
    "for item in rel:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始构建pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分别获取标签列和特征列，进行索引，并进行了重命名。\n",
    "labelIndexer = StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\").fit(df)\n",
    "featureIndexer = VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").fit(df)\n",
    "# featureIndexer: org.apache.spark.ml.feature.VectorIndexerModel = vecIdx_53b988077b38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把数据集随机分成训练集和测试集，其中训练集占70%\n",
    "trainingData, testData = df.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression parameters:\n",
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0, current: 0.8)\n",
      "family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n",
      "featuresCol: features column name. (default: features, current: indexedFeatures)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label, current: indexedLabel)\n",
      "lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n",
      "maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n",
      "rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0, current: 0.3)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n",
      "thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n",
      "upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "# 设置logistic的参数，这里我们统一用setter的方法来设置，也可以用ParamMap来设置，设置了循环次数为10次，正则化项为0.3等，具体的可以设置的参数可以通过explainParams()来获取，还能看到我们已经设置的参数的结果\n",
    "lr = LogisticRegression().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\").setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8)\n",
    "print(\"LogisticRegression parameters:\\n\" + lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置一个labelConverter，目的是把预测的类别重新转化成字符型的\n",
    "labelConverter = IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 构建pipeline，设置stage，然后调用fit()来训练模型。\n",
    "lrPipeline =  Pipeline().setStages([labelIndexer, featureIndexer, lr, labelConverter])\n",
    "lrPipelineModel = lrPipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline本质上是一个Estimator，当pipeline调用fit()的时候就产生了一个PipelineModel，本质上是一个Transformer。然后这个PipelineModel就可以调用transform()来进行预测，生成一个新的DataFrame，即利用训练得到的模型对测试集进行验证。\n",
    "lrPredictions = lrPipelineModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris-versicolor,[5.5,2.4,3.7,1.0]-->prob=[0.608831311288059,0.39116868871194105],predictedLabelIris-versicolor\n",
      "Iris-virginica,[5.6,2.8,4.9,2.0]-->prob=[0.4001648611050161,0.5998351388949839],predictedLabelIris-virginica\n",
      "Iris-versicolor,[5.6,3.0,4.5,1.5]-->prob=[0.505221948790782,0.494778051209218],predictedLabelIris-versicolor\n",
      "Iris-virginica,[5.7,2.5,5.0,2.0]-->prob=[0.4011621306493747,0.5988378693506252],predictedLabelIris-virginica\n",
      "Iris-versicolor,[5.7,2.8,4.5,1.3]-->prob=[0.5486724476582792,0.45132755234172084],predictedLabelIris-versicolor\n",
      "Iris-versicolor,[5.8,2.6,4.0,1.2]-->prob=[0.570672297739129,0.4293277022608709],predictedLabelIris-versicolor\n",
      "Iris-versicolor,[6.1,3.0,4.6,1.4]-->prob=[0.5316543395332591,0.468345660466741],predictedLabelIris-versicolor\n",
      "Iris-virginica,[6.1,3.0,4.9,1.8]-->prob=[0.446765897766743,0.553234102233257],predictedLabelIris-virginica\n",
      "Iris-versicolor,[6.2,2.2,4.5,1.5]-->prob=[0.511449640933197,0.488550359066803],predictedLabelIris-versicolor\n",
      "Iris-virginica,[6.2,2.8,4.8,1.8]-->prob=[0.4477926049097338,0.5522073950902662],predictedLabelIris-virginica\n",
      "Iris-versicolor,[6.2,2.9,4.3,1.3]-->prob=[0.5538091325620049,0.446190867437995],predictedLabelIris-versicolor\n",
      "Iris-virginica,[6.3,2.8,5.1,1.5]-->prob=[0.5124872963673683,0.48751270363263155],predictedLabelIris-versicolor\n",
      "Iris-virginica,[6.3,2.9,5.6,1.8]-->prob=[0.4488197573647022,0.5511802426352976],predictedLabelIris-virginica\n",
      "Iris-versicolor,[6.3,3.3,4.7,1.6]-->prob=[0.4912074606302918,0.5087925393697083],predictedLabelIris-virginica\n",
      "Iris-versicolor,[6.4,2.9,4.3,1.3]-->prob=[0.55586065586705,0.44413934413295003],predictedLabelIris-versicolor\n",
      "Iris-virginica,[6.4,3.1,5.5,1.8]-->prob=[0.4498473465578554,0.5501526534421446],predictedLabelIris-virginica\n",
      "Iris-versicolor,[6.4,3.2,4.5,1.5]-->prob=[0.5135248441821171,0.4864751558178829],predictedLabelIris-versicolor\n",
      "Iris-virginica,[6.4,3.2,5.3,2.3]-->prob=[0.3482014648553396,0.6517985351446605],predictedLabelIris-virginica\n",
      "Iris-versicolor,[6.7,3.0,5.0,1.7]-->prob=[0.4740994217424705,0.5259005782575294],predictedLabelIris-virginica\n",
      "Iris-versicolor,[6.7,3.1,4.7,1.5]-->prob=[0.5166367526370118,0.48336324736298814],predictedLabelIris-versicolor\n",
      "Iris-virginica,[6.7,3.3,5.7,2.1]-->prob=[0.39073385038581865,0.6092661496141814],predictedLabelIris-virginica\n",
      "Iris-versicolor,[6.8,2.8,4.8,1.4]-->prob=[0.5388858015150211,0.4611141984849789],predictedLabelIris-versicolor\n",
      "Iris-virginica,[6.9,3.2,5.7,2.3]-->prob=[0.3529289660777292,0.6470710339222707],predictedLabelIris-virginica\n",
      "Iris-virginica,[7.1,3.0,5.9,2.1]-->prob=[0.3946956208940087,0.6053043791059913],predictedLabelIris-virginica\n",
      "Iris-virginica,[7.2,3.0,5.8,1.6]-->prob=[0.5005508102316143,0.49944918976838587],predictedLabelIris-versicolor\n",
      "Iris-virginica,[7.2,3.2,6.0,1.8]-->prob=[0.45808274906016216,0.5419172509398379],predictedLabelIris-virginica\n",
      "Iris-virginica,[7.6,3.0,6.6,2.1]-->prob=[0.3996673035785165,0.6003326964214835],predictedLabelIris-virginica\n",
      "Iris-virginica,[7.7,2.8,6.7,2.0]-->prob=[0.42126924767926,0.5787307523207401],predictedLabelIris-virginica\n"
     ]
    }
   ],
   "source": [
    "# 出预测的结果，其中select选择要输出的列，collect获取所有行的数据，用foreach把每行打印出来。其中打印出来的值依次分别代表该行数据的真实分类和特征值、预测属于不同分类的概率、预测的分类。\n",
    "preRel = lrPredictions.select(\"predictedLabel\", \"label\", \"features\", \"probability\").collect()\n",
    "for item in preRel:\n",
    "    print(str(item['label'])+','+str(item['features'])+'-->prob='+str(item['probability'])+',predictedLabel'+str(item['predictedLabel']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.1428571428571428\n"
     ]
    }
   ],
   "source": [
    "# 创建一个MulticlassClassificationEvaluator实例，用setter方法把预测分类的列名和真实分类的列名进行设置；然后计算预测准确率和错误率\n",
    "evaluator = MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\")\n",
    "lrAccuracy = evaluator.evaluate(lrPredictions)\n",
    "print(\"Test Error = \" + str(1.0 - lrAccuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-0.04153002805609655,0.0,0.0,0.0851333577302661]Intercept: -0.21398718619541965numClasses: 2numFeatures: 4\n"
     ]
    }
   ],
   "source": [
    "# 通过model来获取我们训练得到的逻辑斯蒂模型。前面已经说过model是一个PipelineModel，因此我们可以通过调用它的stages来获取模型，具体如下\n",
    "lrModel = lrPipelineModel.stages[2]\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients)+\"Intercept: \"+str(lrModel.intercept)+\"numClasses: \"+str(lrModel.numClasses)+\"numFeatures: \"+str(lrModel.numFeatures))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6927613284609757\n",
      "0.6898340245452709\n",
      "0.6877084111664772\n",
      "0.6867085352031355\n",
      "0.6799440366541055\n",
      "0.6702999584351561\n",
      "0.6656403590810849\n",
      "0.6644955913087869\n",
      "0.6631918088409949\n",
      "0.6624783614222991\n",
      "0.6615311087179351\n",
      "------------------------------\n",
      "0.9826254826254825\n",
      "0.9565217391304348\n",
      "-----------------\n",
      "0.5408861192767206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression_4373b45d07955900227e"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark的ml库还提供了一个对模型的摘要总结（summary），不过目前只支持二项逻辑斯蒂回归，而且要显式转化成BinaryLogisticRegressionSummary\n",
    "# 首先获得二项逻辑斯模型的摘要\n",
    "trainingSummary = lrModel.summary\n",
    "# 获得10次循环中损失函数的变化，并将结果打印出来，可以看到损失函数随着循环是逐渐变小的\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "for item in objectiveHistory:\n",
    "    print(item)\n",
    "print(\"------------------------------\")\n",
    "# 把摘要强制转化为BinaryLogisticRegressionSummary ，来获取用来评估模型性能的矩阵，通过获取ROC，我们可以判断模型的好坏，areaUnderROC达到了 0.969551282051282，说明我们的分类器还是不错的\n",
    "print(trainingSummary.areaUnderROC)\n",
    "\n",
    "# 通过最大化fMeasure来选取最合适的阈值，其中fMeasure是一个综合了召回率和准确率的指标，通过最大化fMeasure，我们可以选取到用来分类的最合适的阈值。\n",
    "fMeasure = trainingSummary.fMeasureByThreshold\n",
    "maxFMeasure = fMeasure.select(functions.max(\"F-Measure\")).head()[0] \n",
    "print(maxFMeasure)\n",
    "print(\"-----------------\")\n",
    "bestThreshold = fMeasure.where(fMeasure[\"F-Measure\"]== maxFMeasure).select(\"threshold\").head()[0]\n",
    "print(bestThreshold)\n",
    "lr.setThreshold(bestThreshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iris-versicolor,[5.5,2.4,3.7,1.0])-->prob=[0.6190870833392402,0.3809129166607598],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[5.6,2.8,4.9,2.0])-->prob=[0.3899902744026083,0.6100097255973916],predictLabel=Iris-virginica\n",
      "(Iris-versicolor,[5.6,3.0,4.5,1.5])-->prob=[0.5056604423154984,0.4943395576845015],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[5.7,2.5,5.0,2.0])-->prob=[0.39164811792444604,0.608351882075554],predictLabel=Iris-virginica\n",
      "(Iris-versicolor,[5.7,2.8,4.5,1.3])-->prob=[0.5541874662627947,0.4458125337372053],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[5.8,2.6,4.0,1.2])-->prob=[0.5789749933959147,0.4210250066040854],predictLabel=Iris-versicolor\n",
      "(Iris-versicolor,[6.1,3.0,4.6,1.4])-->prob=[0.5377925194267595,0.46220748057324046],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[6.1,3.0,4.9,1.8])-->prob=[0.4441002327295816,0.5558997672704183],predictLabel=Iris-virginica\n",
      "(Iris-versicolor,[6.2,2.2,4.5,1.5])-->prob=[0.516100230459997,0.4838997695400031],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[6.2,2.8,4.8,1.8])-->prob=[0.4458199884473104,0.5541800115526895],predictLabel=Iris-virginica\n",
      "(Iris-versicolor,[6.2,2.9,4.3,1.3])-->prob=[0.5627724315662106,0.43722756843378935],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[6.3,2.8,5.1,1.5])-->prob=[0.5178390758479221,0.4821609241520779],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[6.3,2.9,5.6,1.8])-->prob=[0.4475410423002122,0.5524589576997878],predictLabel=Iris-virginica\n",
      "(Iris-versicolor,[6.3,3.3,4.7,1.6])-->prob=[0.49434710001388804,0.5056528999861121],predictLabel=Iris-virginica\n",
      "(Iris-versicolor,[6.4,2.9,4.3,1.3])-->prob=[0.5661962109511951,0.433803789048805],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[6.4,3.1,5.5,1.8])-->prob=[0.4492633539860792,0.5507366460139208],predictLabel=Iris-virginica\n",
      "(Iris-versicolor,[6.4,3.2,4.5,1.5])-->prob=[0.5195774892897475,0.4804225107102526],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[6.4,3.2,5.3,2.3])-->prob=[0.33768152023216347,0.6623184797678365],predictLabel=Iris-virginica\n",
      "(Iris-versicolor,[6.7,3.0,5.0,1.7])-->prob=[0.4778250328916288,0.5221749671083712],predictLabel=Iris-virginica\n",
      "(Iris-versicolor,[6.7,3.1,4.7,1.5])-->prob=[0.5247897184764578,0.4752102815235422],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[6.7,3.3,5.7,2.1])-->prob=[0.3858584703155811,0.6141415296844188],predictLabel=Iris-virginica\n",
      "(Iris-versicolor,[6.8,2.8,4.8,1.4])-->prob=[0.549884197835838,0.45011580216416214],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[6.9,3.2,5.7,2.3])-->prob=[0.34551190945390803,0.6544880905460919],predictLabel=Iris-virginica\n",
      "(Iris-virginica,[7.1,3.0,5.9,2.1])-->prob=[0.3924796169215919,0.607520383078408],predictLabel=Iris-virginica\n",
      "(Iris-virginica,[7.2,3.0,5.8,1.6])-->prob=[0.510013191780789,0.48998680821921103],predictLabel=Iris-versicolor\n",
      "(Iris-virginica,[7.2,3.2,6.0,1.8])-->prob=[0.46308225533992664,0.5369177446600732],predictLabel=Iris-virginica\n",
      "(Iris-virginica,[7.6,3.0,6.6,2.1])-->prob=[0.4008117217074887,0.5991882782925114],predictLabel=Iris-virginica\n",
      "(Iris-virginica,[7.7,2.8,6.7,2.0])-->prob=[0.42528334662927353,0.5747166533707264],predictLabel=Iris-virginica\n",
      "Test Error = 0.1428571428571428\n",
      "Multinomial coefficients: DenseMatrix([[ 0.03481705,  0.        ,  0.        , -0.04699958],\n",
      "             [-0.03481705,  0.        ,  0.        ,  0.04699958]])Multin omial intercepts: [0.05134381771542905,-0.05134381771542905]numClasses: 2numFeatures: 4\n"
     ]
    }
   ],
   "source": [
    "# 用多项逻辑斯蒂回归解决 二分类 问题\n",
    "# 对于二分类问题，我们还可以用多项逻辑斯蒂回归进行多分类分析。多项逻辑斯蒂回归与二项逻辑斯蒂回归类似，只是在模型设置上把family参数设置成multinomial，\n",
    "mlr =  LogisticRegression().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\").setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8).setFamily(\"multinomial\")\n",
    "mlrPipeline = Pipeline().setStages([labelIndexer, featureIndexer, mlr, labelConverter])\n",
    "mlrPipelineModel = mlrPipeline.fit(trainingData)\n",
    "mlrPredictions = mlrPipelineModel.transform(testData)\n",
    "mlrPreRel = mlrPredictions.select(\"predictedLabel\", \"label\", \"features\", \"probability\").collect()\n",
    "for item in mlrPreRel:\n",
    "    print('('+str(item['label'])+','+str(item['features'])+')-->prob='+str(item['probability'])+',predictLabel='+str(item['predictedLabel']))\n",
    "mlrAccuracy = evaluator.evaluate(mlrPredictions)\n",
    "print(\"Test Error = \" + str(1.0 - mlrAccuracy)) \n",
    "mlrModel = mlrPipelineModel.stages[2]\n",
    "print(\"Multinomial coefficients: \" +str(mlrModel.coefficientMatrix)+\"Multin omial intercepts: \"+str(mlrModel.interceptVector)+\"numClasses: \"+str(mlrModel.numClasses)+\n",
    "\"numFeatures: \"+str(mlrModel.numFeatures))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用多项逻辑斯蒂回归解决 多分类 问题\n",
    "# 对于多分类问题，我们需要用多项逻辑斯蒂回归进行多分类分析。这里我们用全部的iris数据集，即有三个类别，过程与上述基本一致"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
