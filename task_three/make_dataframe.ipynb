{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark --conf spark.executor.extraClassPath=mysql-connector-java-8.0.17.jar --driver-class-path mysql-connector-java-8.0.17.jar --jars mysql-connector-java-8.0.17.jar\n",
    "import os\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"python3\"\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars file:///home/mayfly/workstation/mycode/spark_study/mysql-connector-java-8.0.17.jar pyspark-shell'\n",
    "# os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"python3\"\n",
    "# os.environ['SPARK_HOME'] = \"/home/mayfly/workstation/env/spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+------+---+\n",
      "| id|    name|gender|age|\n",
      "+---+--------+------+---+\n",
      "|  1| Xueqian|     F| 23|\n",
      "|  2|Weiliang|     M| 24|\n",
      "+---+--------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.getOrCreate()\n",
    "password = \"password\"\n",
    "jdbcDF = spark.read.format(\"jdbc\").option(\"url\", \"jdbc:mysql://localhost:3306/spark\")\\\n",
    "    .option(\"driver\",\"com.mysql.jdbc.Driver\").option(\"dbtable\", \"student\")\\\n",
    "    .option(\"user\", \"root\").option(\"password\", password).load()\n",
    "jdbcDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import IntegerType\n",
    "studentRDD = spark.sparkContext.parallelize([\"3 Rongcheng M 26\",\"4 Guanhua M 27\"]).map(lambda line : line.split(\" \"))\n",
    "# 下面要设置模式信息\n",
    "schema = StructType([StructField(\"name\", StringType(), True),StructField(\"gender\", StringType(), True),StructField(\"age\",IntegerType(), True)])\n",
    "rowRDD = studentRDD.map(lambda p : Row(p[1].strip(), p[2].strip(),int(p[3])))\n",
    "# 建立起Row对象和模式之间的对应关系，也就是把数据和模式对应起来\n",
    "studentDF = spark.createDataFrame(rowRDD, schema)\n",
    "prop = {}\n",
    "prop['user'] = 'root'\n",
    "prop['password'] = password\n",
    "prop['driver'] = \"com.mysql.jdbc.Driver\"\n",
    "studentDF.write.jdbc(\"jdbc:mysql://localhost:3306/spark\",'student','append', prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: 29,Age:Michael\n",
      "Name: 30,Age:Andy\n",
      "Name: 19,Age:Justin\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import Row\n",
    "def f(x):\n",
    "    rel = {}\n",
    "    rel['name'] = x[0]\n",
    "    rel['age'] = x[1]\n",
    "    return rel\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "# sc = SparkContext(appName=\"make_rdd\", master=\"local\")\n",
    "sc = spark.sparkContext\n",
    "peopleDF = sc.textFile(\"file:///home/mayfly/workstation/mycode/spark_study/people.txt\").map(lambda line : line.split(',')).map(lambda x: Row(**f(x))).toDF()\n",
    "peopleDF.createOrReplaceTempView(\"people\")  #必须注册为临时表才能供下面的查询使用 \n",
    "personsDF = spark.sql(\"select * from people\")\n",
    "# personsDF.rdd.map(lambda t : \"Name:\"+t[0]+\",\"+\"Age:\"+t[1]).foreach(print)\n",
    "for t in personsDF.rdd.map(lambda t : \"Name:\"+t[0]+\",\"+\"Age:\"+t[1]).collect():\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructField(name,StringType,true), StructField(age,StringType,true)]\n",
      "StructType(List(StructField(name,StringType,true),StructField(age,StringType,true)))\n",
      "[['Michael', ' 29'], ['Andy', ' 30'], ['Justin', ' 19']]\n",
      "[<Row(Michael,  29)>, <Row(Andy,  30)>, <Row(Justin,  19)>]\n",
      "name: Michael,age: 29\n",
      "name: Andy,age: 30\n",
      "name: Justin,age: 19\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import Row\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# sc = SparkContext(appName=\"make_rdd\", master=\"local\")\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "# 生成 RDD\n",
    "peopleRDD = sc.textFile(\"file:///home/mayfly/workstation/mycode/spark_study/people.txt\")\n",
    "\n",
    "# 定义一个模式字符串\n",
    "schemaString = \"name age\"\n",
    " \n",
    "# 根据模式字符串生成模式\n",
    "fields = list(map( lambda fieldName : StructField(fieldName, StringType(), nullable = True), schemaString.split(\" \")))\n",
    "print(fields)\n",
    "schema = StructType(fields)\n",
    "print(schema)\n",
    "# 从上面信息可以看出，schema描述了模式信息，模式中包含name和age两个字段\n",
    " \n",
    " \n",
    "rowRDD = peopleRDD.map(lambda line : line.split(','))\n",
    "print(rowRDD.collect())\n",
    "\n",
    "rowRDD = rowRDD.map(lambda attributes : Row(attributes[0], attributes[1]))\n",
    "print(rowRDD.collect())\n",
    " \n",
    "peopleDF = spark.createDataFrame(rowRDD, schema)\n",
    "\n",
    "# 必须注册为临时表才能供下面查询使用\n",
    "peopleDF.createOrReplaceTempView(\"people\")\n",
    " \n",
    "results = spark.sql(\"SELECT * FROM people\")\n",
    "# results.rdd.saveAsTextFile(\"file:///home/mayfly/workstation/mycode/spark_study/newpeople.txt\")\n",
    "results = results.rdd.map( lambda attributes : \"name: \" + attributes[0]+\",\"+\"age:\"+attributes[1])\n",
    "for t in results.collect():\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| age|   name|\n",
      "+----+-------+\n",
      "|null|Michael|\n",
      "|  30|   Andy|\n",
      "|  19| Justin|\n",
      "+----+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.getOrCreate()\n",
    "df = spark.read.json(\"file:///home/mayfly/workstation/mycode/spark_study/people.json\")\n",
    "print(df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
